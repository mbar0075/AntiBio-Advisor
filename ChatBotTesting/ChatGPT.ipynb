{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# import sys\n",
    "\n",
    "# # Set your API key\n",
    "# openai.api_key = \"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\"\n",
    "#\"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\"\n",
    "\n",
    "# # Define the conversation\n",
    "# conversation = [\n",
    "#     # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Can you tell me a joke?\"}\n",
    "#     # {\"role\": \"assistant\", \"content\": \"I'm not sure. Where are you located?\"}\n",
    "# ]\n",
    "\n",
    "# # Make the API call\n",
    "# response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=conversation\n",
    "# )\n",
    "\n",
    "# # Extract the assistant's reply\n",
    "# assistant_reply = response['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "Hi there! How can I assist you today?\n",
      "\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# add assisant section of conversation\n",
    "\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "userQueries = []\n",
    "\n",
    "while True:\n",
    "    userQueries.append(input())\n",
    "    print(userQueries[len(userQueries)-1])\n",
    "\n",
    "    # Define the conversation\n",
    "\n",
    "    conversation = []\n",
    "\n",
    "    for prompt in userQueries:\n",
    "        conversation.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Make the API call\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=conversation\n",
    "    )\n",
    "\n",
    "    # Extract the assistant's reply\n",
    "    print(response['choices'][0]['message']['content'])\n",
    "\n",
    "    if userQueries[len(userQueries)-1].lower() == \"exit\":\n",
    "        sys.exit(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response :::  {\n",
      "  \"object\": \"file\",\n",
      "  \"id\": \"file-xjYdDHwZOfQWV7K1ildvwC7j\",\n",
      "  \"purpose\": \"fine-tune\",\n",
      "  \"filename\": \"file\",\n",
      "  \"bytes\": 956,\n",
      "  \"created_at\": 1697098555,\n",
      "  \"status\": \"uploaded\",\n",
      "  \"status_details\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set your API key\n",
    "api_key = \"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\"\n",
    "openai.api_key = api_key\n",
    "\n",
    "def upload_file():\n",
    "    response = upload_file_to_openai()\n",
    "    print(\"response ::: \", response)\n",
    "\n",
    "def upload_file_to_openai():\n",
    "    with open(\"sample_json.txt\", \"rb\") as file:\n",
    "        response = openai.File.create(\n",
    "            file=file,\n",
    "            purpose=\"fine-tune\"\n",
    "        )\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    upload_file()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response :::  {\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"id\": \"ft-NaldvollaFKWZg5TwAt5wfs3\",\n",
      "  \"hyperparams\": {\n",
      "    \"n_epochs\": 4,\n",
      "    \"batch_size\": null,\n",
      "    \"prompt_loss_weight\": 0.01,\n",
      "    \"learning_rate_multiplier\": null\n",
      "  },\n",
      "  \"organization_id\": \"org-itgWcRs1eipZqDGjocuvzPQO\",\n",
      "  \"model\": \"curie\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"object\": \"file\",\n",
      "      \"id\": \"file-dVWyBB7mHqkrYtAnil1s5z4h\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"file\",\n",
      "      \"bytes\": 289,\n",
      "      \"created_at\": 1697004615,\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"validation_files\": [],\n",
      "  \"result_files\": [],\n",
      "  \"created_at\": 1697049827,\n",
      "  \"updated_at\": 1697049827,\n",
      "  \"status\": \"pending\",\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"object\": \"fine-tune-event\",\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-NaldvollaFKWZg5TwAt5wfs3\",\n",
      "      \"created_at\": 1697049827\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set your API key\n",
    "api_key = \"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\"\n",
    "openai.api_key = api_key\n",
    "\n",
    "def create_fine_tune():\n",
    "    response = openai.FineTune.create(\n",
    "        training_file=\"file-dVWyBB7mHqkrYtAnil1s5z4h\"\n",
    "    )\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    response = create_fine_tune()\n",
    "    print(\"response ::: \", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "al \\\\ A x\n",
      "\n",
      "_ MEDIFIRST\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To make it work first run the pip command then go to their github and run the 64bit .exe, take note of the saved path location and replace the path in the code below \n",
    "# pip install pytesseract\n",
    "# https://github.com/UB-Mannheim/tesseract/wiki\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n",
    "\n",
    "# Open an image using Pillow\n",
    "image = Image.open(\"../img4.jpg\")  \n",
    "\n",
    "# Use pytesseract to extract text from the image\n",
    "text = pytesseract.image_to_string(image, config='--psm 3')\n",
    "\n",
    "# # Print the extracted text\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatBot Testing custome model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n",
      "AD-L25 ;Store CAGE X coord in RAM with SF DATADR2-1,LO\n",
      "\n",
      "SHLDCPYA OR 6,0377 *BHRHLD-L25 ;Load CAGE PETALS\n",
      "What is happening?\n",
      " Why are computers named after 16th century mathematician and courtier Blaise Pascal? Are they named as a result of common family names or do they do so as a last minute decision? Why do Apple Macs get their own names, some of\n",
      "The ramblings of a mad man\n",
      ". You wouldn't understand. Please know this.\"\n",
      "\n",
      "\"I won't.\"\n",
      "\n",
      "He smiled, relieved. He was having a better reaction to her every minute. Help me Delko, he pleaded. Help me.\n",
      "\n",
      "\n",
      "* *\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add assisant section of conversation\n",
    "\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "userQueries = []\n",
    "\n",
    "while True:\n",
    "    userQueries.append(input())\n",
    "    print(userQueries[len(userQueries)-1])\n",
    "\n",
    "    # Define the conversation\n",
    "\n",
    "    conversation = []\n",
    "\n",
    "    for prompt in userQueries:\n",
    "        conversation.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Make the API call\n",
    "    response = openai.Completion.create(\n",
    "        model=\"curie:ft-personal-2023-10-11-18-45-14\",\n",
    "        prompt = userQueries[len(userQueries)-1],\n",
    "        max_tokens = 50\n",
    "    )\n",
    "\n",
    "    # Extract the assistant's reply\n",
    "    print(response.choices[0].text)\n",
    "\n",
    "    if userQueries[len(userQueries)-1].lower() == \"exit\":\n",
    "        sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing lib: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\GitHubRepository\\AntiBio-Advisor\\ChatBotTesting\\ChatGPT.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/GitHubRepository/AntiBio-Advisor/ChatBotTesting/ChatGPT.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mst\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/GitHubRepository/AntiBio-Advisor/ChatBotTesting/ChatGPT.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mreplicate\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/GitHubRepository/AntiBio-Advisor/ChatBotTesting/ChatGPT.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\streamlit\\__init__.py:55\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m# Give the package a version.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m __version__ \u001b[39m=\u001b[39m _STREAMLIT_VERSION_STRING\n\u001b[1;32m---> 55\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdelta_generator\u001b[39;00m \u001b[39mimport\u001b[39;00m DeltaGenerator \u001b[39mas\u001b[39;00m _DeltaGenerator\n\u001b[0;32m     56\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mRootContainer_pb2\u001b[39;00m \u001b[39mimport\u001b[39;00m RootContainer \u001b[39mas\u001b[39;00m _RootContainer\n\u001b[0;32m     57\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcaching\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     58\u001b[0m     cache_resource \u001b[39mas\u001b[39;00m _cache_resource,\n\u001b[0;32m     59\u001b[0m     cache_data \u001b[39mas\u001b[39;00m _cache_data,\n\u001b[0;32m     60\u001b[0m     experimental_singleton \u001b[39mas\u001b[39;00m _experimental_singleton,\n\u001b[0;32m     61\u001b[0m     experimental_memo \u001b[39mas\u001b[39;00m _experimental_memo,\n\u001b[0;32m     62\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\streamlit\\delta_generator.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mclick\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping_extensions\u001b[39;00m \u001b[39mimport\u001b[39;00m Final, Literal\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m \u001b[39mimport\u001b[39;00m config, cursor, env_util, logger, runtime, type_util, util\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcursor\u001b[39;00m \u001b[39mimport\u001b[39;00m Cursor\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39melements\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39malert\u001b[39;00m \u001b[39mimport\u001b[39;00m AlertMixin\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\streamlit\\cursor.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, List, Optional, Tuple\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m \u001b[39mimport\u001b[39;00m util\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mscriptrunner\u001b[39;00m \u001b[39mimport\u001b[39;00m get_script_run_ctx\n\u001b[0;32m     21\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake_delta_path\u001b[39m(\n\u001b[0;32m     22\u001b[0m     root_container: \u001b[39mint\u001b[39m, parent_path: Tuple[\u001b[39mint\u001b[39m, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m], index: \u001b[39mint\u001b[39m\n\u001b[0;32m     23\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[\u001b[39mint\u001b[39m]:\n\u001b[0;32m     24\u001b[0m     delta_path \u001b[39m=\u001b[39m [root_container]\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\__init__.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[39m# Explicitly re-export public symbols from runtime.py and session_manager.py\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m \u001b[39mimport\u001b[39;00m Runtime \u001b[39mas\u001b[39;00m Runtime\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m \u001b[39mimport\u001b[39;00m RuntimeConfig \u001b[39mas\u001b[39;00m RuntimeConfig\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m \u001b[39mimport\u001b[39;00m RuntimeState \u001b[39mas\u001b[39;00m RuntimeState\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\runtime.py:30\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mBackMsg_pb2\u001b[39;00m \u001b[39mimport\u001b[39;00m BackMsg\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mForwardMsg_pb2\u001b[39;00m \u001b[39mimport\u001b[39;00m ForwardMsg\n\u001b[1;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapp_session\u001b[39;00m \u001b[39mimport\u001b[39;00m AppSession\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcaching\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     32\u001b[0m     get_data_cache_stats_provider,\n\u001b[0;32m     33\u001b[0m     get_resource_cache_stats_provider,\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcaching\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstorage\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlocal_disk_cache_storage\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     36\u001b[0m     LocalDiskCacheStorageManager,\n\u001b[0;32m     37\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\app_session.py:36\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mNewSession_pb2\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     30\u001b[0m     Config,\n\u001b[0;32m     31\u001b[0m     CustomThemeConfig,\n\u001b[0;32m     32\u001b[0m     NewSession,\n\u001b[0;32m     33\u001b[0m     UserInfo,\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mPagesChanged_pb2\u001b[39;00m \u001b[39mimport\u001b[39;00m PagesChanged\n\u001b[1;32m---> 36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m \u001b[39mimport\u001b[39;00m caching, legacy_caching\n\u001b[0;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mforward_msg_queue\u001b[39;00m \u001b[39mimport\u001b[39;00m ForwardMsgQueue\n\u001b[0;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics_util\u001b[39;00m \u001b[39mimport\u001b[39;00m Installation\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\caching\\__init__.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmessage\u001b[39;00m \u001b[39mimport\u001b[39;00m Message\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mproto\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mBlock_pb2\u001b[39;00m \u001b[39mimport\u001b[39;00m Block\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcaching\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcache_data_api\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     22\u001b[0m     CACHE_DATA_MESSAGE_REPLAY_CTX,\n\u001b[0;32m     23\u001b[0m     CacheDataAPI,\n\u001b[0;32m     24\u001b[0m     _data_caches,\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcaching\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcache_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m CACHE_DOCS_URL \u001b[39mas\u001b[39;00m CACHE_DOCS_URL\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcaching\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcache_resource_api\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     CACHE_RESOURCE_MESSAGE_REPLAY_CTX,\n\u001b[0;32m     29\u001b[0m     CacheResourceAPI,\n\u001b[0;32m     30\u001b[0m     _resource_caches,\n\u001b[0;32m     31\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\caching\\cache_data_api.py:32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m StreamlitAPIException\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlogger\u001b[39;00m \u001b[39mimport\u001b[39;00m get_logger\n\u001b[1;32m---> 32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcaching\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcache_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m CacheError, CacheKeyNotFoundError\n\u001b[0;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcaching\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcache_type\u001b[39;00m \u001b[39mimport\u001b[39;00m CacheType\n\u001b[0;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcaching\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcache_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     35\u001b[0m     Cache,\n\u001b[0;32m     36\u001b[0m     CachedFuncInfo,\n\u001b[0;32m     37\u001b[0m     make_cached_func_wrapper,\n\u001b[0;32m     38\u001b[0m     ttl_to_seconds,\n\u001b[0;32m     39\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\caching\\cache_errors.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtypes\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Any, Optional\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m \u001b[39mimport\u001b[39;00m type_util\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39merrors\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     MarkdownFormattedException,\n\u001b[0;32m     21\u001b[0m     StreamlitAPIException,\n\u001b[0;32m     22\u001b[0m     StreamlitAPIWarning,\n\u001b[0;32m     23\u001b[0m )\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstreamlit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcaching\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcache_type\u001b[39;00m \u001b[39mimport\u001b[39;00m CacheType, get_decorator_api_name\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\streamlit\\type_util.py:42\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     TYPE_CHECKING,\n\u001b[0;32m     26\u001b[0m     Any,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     overload,\n\u001b[0;32m     39\u001b[0m )\n\u001b[0;32m     41\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyarrow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpa\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m DataFrame, Index, MultiIndex, Series\n\u001b[0;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapi\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m infer_dtype, is_dict_like, is_list_like\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\pyarrow\\__init__.py:65\u001b[0m\n\u001b[0;32m     63\u001b[0m _gc_enabled \u001b[39m=\u001b[39m _gc\u001b[39m.\u001b[39misenabled()\n\u001b[0;32m     64\u001b[0m _gc\u001b[39m.\u001b[39mdisable()\n\u001b[1;32m---> 65\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyarrow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_lib\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[39mif\u001b[39;00m _gc_enabled:\n\u001b[0;32m     67\u001b[0m     _gc\u001b[39m.\u001b[39menable()\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing lib: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import replicate\n",
    "import os\n",
    "\n",
    "# App title\n",
    "st.set_page_config(page_title=\"ü¶ôüí¨ Llama 2 Chatbot\")\n",
    "\n",
    "# Replicate Credentials\n",
    "with st.sidebar:\n",
    "    st.title('ü¶ôüí¨ Llama 2 Chatbot')\n",
    "    if 'REPLICATE_API_TOKEN' in st.secrets:\n",
    "        st.success('API key already provided!', icon='‚úÖ')\n",
    "        replicate_api = st.secrets['REPLICATE_API_TOKEN']\n",
    "    else:\n",
    "        replicate_api = st.text_input('Enter Replicate API token:', type='password')\n",
    "        if not (replicate_api.startswith('r8_') and len(replicate_api)==40):\n",
    "            st.warning('Please enter your credentials!', icon='‚ö†Ô∏è')\n",
    "        else:\n",
    "            st.success('Proceed to entering your prompt message!', icon='üëâ')\n",
    "    os.environ['REPLICATE_API_TOKEN'] = replicate_api\n",
    "\n",
    "    st.subheader('Models and parameters')\n",
    "    selected_model = st.sidebar.selectbox('Choose a Llama2 model', ['Llama2-7B', 'Llama2-13B'], key='selected_model')\n",
    "    if selected_model == 'Llama2-7B':\n",
    "        llm = 'a16z-infra/llama7b-v2-chat:4f0a4744c7295c024a1de15e1a63c880d3da035fa1f49bfd344fe076074c8eea'\n",
    "    elif selected_model == 'Llama2-13B':\n",
    "        llm = 'a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5'\n",
    "    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
    "    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
    "    max_length = st.sidebar.slider('max_length', min_value=32, max_value=128, value=120, step=8)\n",
    "    st.markdown('üìñ Learn how to build this app in this [blog](https://blog.streamlit.io/how-to-build-a-llama-2-chatbot/)!')\n",
    "\n",
    "# Store LLM generated responses\n",
    "if \"messages\" not in st.session_state.keys():\n",
    "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
    "\n",
    "# Display or clear chat messages\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.write(message[\"content\"])\n",
    "\n",
    "def clear_chat_history():\n",
    "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"How may I assist you today?\"}]\n",
    "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
    "\n",
    "# Function for generating LLaMA2 response. Refactored from https://github.com/a16z-infra/llama2-chatbot\n",
    "def generate_llama2_response(prompt_input):\n",
    "    string_dialogue = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'.\"\n",
    "    for dict_message in st.session_state.messages:\n",
    "        if dict_message[\"role\"] == \"user\":\n",
    "            string_dialogue += \"User: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
    "        else:\n",
    "            string_dialogue += \"Assistant: \" + dict_message[\"content\"] + \"\\n\\n\"\n",
    "    output = replicate.run('a16z-infra/llama13b-v2-chat:df7690f1994d94e96ad9d568eac121aecf50684a0b0963b25a41cc40061269e5', \n",
    "                           input={\"prompt\": f\"{string_dialogue} {prompt_input} Assistant: \",\n",
    "                                  \"temperature\":temperature, \"top_p\":top_p, \"max_length\":max_length, \"repetition_penalty\":1})\n",
    "    return output\n",
    "\n",
    "# User-provided prompt\n",
    "if prompt := st.chat_input(disabled=not replicate_api):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.write(prompt)\n",
    "\n",
    "# Generate a new response if last message is not from assistant\n",
    "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        with st.spinner(\"Thinking...\"):\n",
    "            response = generate_llama2_response(prompt)\n",
    "            placeholder = st.empty()\n",
    "            full_response = ''\n",
    "            for item in response:\n",
    "                full_response += item\n",
    "                placeholder.markdown(full_response)\n",
    "            placeholder.markdown(full_response)\n",
    "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
    "    st.session_state.messages.append(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
