{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# import sys\n",
    "\n",
    "# # Set your API key\n",
    "# openai.api_key = \"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\"\n",
    "#\"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\"\n",
    "\n",
    "# # Define the conversation\n",
    "# conversation = [\n",
    "#     # {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Can you tell me a joke?\"}\n",
    "#     # {\"role\": \"assistant\", \"content\": \"I'm not sure. Where are you located?\"}\n",
    "# ]\n",
    "\n",
    "# # Make the API call\n",
    "# response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=conversation\n",
    "# )\n",
    "\n",
    "# # Extract the assistant's reply\n",
    "# assistant_reply = response['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "'$.prompt' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\GitHubRepository\\AntiBio-Advisor\\ChatBotTesting\\ChatGPT.ipynb Cell 3\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHubRepository/AntiBio-Advisor/ChatBotTesting/ChatGPT.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     conversation\u001b[39m.\u001b[39mappend({\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: prompt})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHubRepository/AntiBio-Advisor/ChatBotTesting/ChatGPT.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Make the API call\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/GitHubRepository/AntiBio-Advisor/ChatBotTesting/ChatGPT.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mCompletion\u001b[39m.\u001b[39;49mcreate( \u001b[39m#   response = openai.ChatCompletion.create(\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHubRepository/AntiBio-Advisor/ChatBotTesting/ChatGPT.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcurie:ft-personal-2023-10-11-18-45-14\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m#\"gpt-3.5-turbo\",\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHubRepository/AntiBio-Advisor/ChatBotTesting/ChatGPT.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     prompt \u001b[39m=\u001b[39;49m conversation\u001b[39m#messages=conversation\u001b[39;49;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHubRepository/AntiBio-Advisor/ChatBotTesting/ChatGPT.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHubRepository/AntiBio-Advisor/ChatBotTesting/ChatGPT.ipynb#W2sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Extract the assistant's reply\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/GitHubRepository/AntiBio-Advisor/ChatBotTesting/ChatGPT.ipynb#W2sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mprint\u001b[39m(response[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    139\u001b[0m ):\n\u001b[0;32m    140\u001b[0m     (\n\u001b[0;32m    141\u001b[0m         deployment_id,\n\u001b[0;32m    142\u001b[0m         engine,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[0;32m    153\u001b[0m     )\n\u001b[1;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[0;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    157\u001b[0m         url,\n\u001b[0;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[0;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[0;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[0;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:299\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[0;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    280\u001b[0m     method,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m    289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_raw(\n\u001b[0;32m    290\u001b[0m         method\u001b[39m.\u001b[39mlower(),\n\u001b[0;32m    291\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    298\u001b[0m     )\n\u001b[1;32m--> 299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response(result, stream)\n\u001b[0;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:710\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    702\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    703\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response_line(\n\u001b[0;32m    704\u001b[0m             line, result\u001b[39m.\u001b[39mstatus_code, result\u001b[39m.\u001b[39mheaders, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    705\u001b[0m         )\n\u001b[0;32m    706\u001b[0m         \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m parse_stream(result\u001b[39m.\u001b[39miter_lines())\n\u001b[0;32m    707\u001b[0m     ), \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    708\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    709\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m--> 710\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpret_response_line(\n\u001b[0;32m    711\u001b[0m             result\u001b[39m.\u001b[39;49mcontent\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    712\u001b[0m             result\u001b[39m.\u001b[39;49mstatus_code,\n\u001b[0;32m    713\u001b[0m             result\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    714\u001b[0m             stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    715\u001b[0m         ),\n\u001b[0;32m    716\u001b[0m         \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    717\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py:775\u001b[0m, in \u001b[0;36mAPIRequestor._interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    773\u001b[0m stream_error \u001b[39m=\u001b[39m stream \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m resp\u001b[39m.\u001b[39mdata\n\u001b[0;32m    774\u001b[0m \u001b[39mif\u001b[39;00m stream_error \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m200\u001b[39m \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m rcode \u001b[39m<\u001b[39m \u001b[39m300\u001b[39m:\n\u001b[1;32m--> 775\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_error_response(\n\u001b[0;32m    776\u001b[0m         rbody, rcode, resp\u001b[39m.\u001b[39mdata, rheaders, stream_error\u001b[39m=\u001b[39mstream_error\n\u001b[0;32m    777\u001b[0m     )\n\u001b[0;32m    778\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: '$.prompt' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference."
     ]
    }
   ],
   "source": [
    "# add assisant section of conversation\n",
    "\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "userQueries = []\n",
    "\n",
    "while True:\n",
    "    userQueries.append(input())\n",
    "    print(userQueries[len(userQueries)-1])\n",
    "\n",
    "    # Define the conversation\n",
    "\n",
    "    conversation = []\n",
    "\n",
    "    for prompt in userQueries:\n",
    "        conversation.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Make the API call\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=conversation\n",
    "    )\n",
    "\n",
    "    # Extract the assistant's reply\n",
    "    print(response['choices'][0]['message']['content'])\n",
    "\n",
    "    if userQueries[len(userQueries)-1].lower() == \"exit\":\n",
    "        sys.exit(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response :::  {\n",
      "  \"object\": \"file\",\n",
      "  \"id\": \"file-b3ihemPKloBbqFmnRsmqdxOQ\",\n",
      "  \"purpose\": \"fine-tune\",\n",
      "  \"filename\": \"file\",\n",
      "  \"bytes\": 956,\n",
      "  \"created_at\": 1697049825,\n",
      "  \"status\": \"uploaded\",\n",
      "  \"status_details\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set your API key\n",
    "api_key = \"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\"\n",
    "openai.api_key = api_key\n",
    "\n",
    "def upload_file():\n",
    "    response = upload_file_to_openai()\n",
    "    print(\"response ::: \", response)\n",
    "\n",
    "def upload_file_to_openai():\n",
    "    with open(\"sample_json.txt\", \"rb\") as file:\n",
    "        response = openai.File.create(\n",
    "            file=file,\n",
    "            purpose=\"fine-tune\"\n",
    "        )\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    upload_file()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response :::  {\n",
      "  \"object\": \"fine-tune\",\n",
      "  \"id\": \"ft-NaldvollaFKWZg5TwAt5wfs3\",\n",
      "  \"hyperparams\": {\n",
      "    \"n_epochs\": 4,\n",
      "    \"batch_size\": null,\n",
      "    \"prompt_loss_weight\": 0.01,\n",
      "    \"learning_rate_multiplier\": null\n",
      "  },\n",
      "  \"organization_id\": \"org-itgWcRs1eipZqDGjocuvzPQO\",\n",
      "  \"model\": \"curie\",\n",
      "  \"training_files\": [\n",
      "    {\n",
      "      \"object\": \"file\",\n",
      "      \"id\": \"file-dVWyBB7mHqkrYtAnil1s5z4h\",\n",
      "      \"purpose\": \"fine-tune\",\n",
      "      \"filename\": \"file\",\n",
      "      \"bytes\": 289,\n",
      "      \"created_at\": 1697004615,\n",
      "      \"status\": \"processed\",\n",
      "      \"status_details\": null\n",
      "    }\n",
      "  ],\n",
      "  \"validation_files\": [],\n",
      "  \"result_files\": [],\n",
      "  \"created_at\": 1697049827,\n",
      "  \"updated_at\": 1697049827,\n",
      "  \"status\": \"pending\",\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"events\": [\n",
      "    {\n",
      "      \"object\": \"fine-tune-event\",\n",
      "      \"level\": \"info\",\n",
      "      \"message\": \"Created fine-tune: ft-NaldvollaFKWZg5TwAt5wfs3\",\n",
      "      \"created_at\": 1697049827\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "# Set your API key\n",
    "api_key = \"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\"\n",
    "openai.api_key = api_key\n",
    "\n",
    "def create_fine_tune():\n",
    "    response = openai.FineTune.create(\n",
    "        training_file=\"file-dVWyBB7mHqkrYtAnil1s5z4h\"\n",
    "    )\n",
    "    return response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    response = create_fine_tune()\n",
    "    print(\"response ::: \", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "al \\\\ A x\n",
      "\n",
      "_ MEDIFIRST\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To make it work first run the pip command then go to their github and run the 64bit .exe, take note of the saved path location and replace the path in the code below \n",
    "# pip install pytesseract\n",
    "# https://github.com/UB-Mannheim/tesseract/wiki\n",
    "\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n",
    "\n",
    "# Open an image using Pillow\n",
    "image = Image.open(\"../img4.jpg\")  \n",
    "\n",
    "# Use pytesseract to extract text from the image\n",
    "text = pytesseract.image_to_string(image, config='--psm 3')\n",
    "\n",
    "# # Print the extracted text\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ChatBot Testing custome model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n",
      "AD-L25 ;Store CAGE X coord in RAM with SF DATADR2-1,LO\n",
      "\n",
      "SHLDCPYA OR 6,0377 *BHRHLD-L25 ;Load CAGE PETALS\n",
      "What is happening?\n",
      " Why are computers named after 16th century mathematician and courtier Blaise Pascal? Are they named as a result of common family names or do they do so as a last minute decision? Why do Apple Macs get their own names, some of\n",
      "The ramblings of a mad man\n",
      ". You wouldn't understand. Please know this.\"\n",
      "\n",
      "\"I won't.\"\n",
      "\n",
      "He smiled, relieved. He was having a better reaction to her every minute. Help me Delko, he pleaded. Help me.\n",
      "\n",
      "\n",
      "* *\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add assisant section of conversation\n",
    "\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "# Set your API key\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "userQueries = []\n",
    "\n",
    "while True:\n",
    "    userQueries.append(input())\n",
    "    print(userQueries[len(userQueries)-1])\n",
    "\n",
    "    # Define the conversation\n",
    "\n",
    "    conversation = []\n",
    "\n",
    "    for prompt in userQueries:\n",
    "        conversation.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Make the API call\n",
    "    response = openai.Completion.create(\n",
    "        model=\"curie:ft-personal-2023-10-11-18-45-14\",\n",
    "        prompt = userQueries[len(userQueries)-1],\n",
    "        max_tokens = 50\n",
    "    )\n",
    "\n",
    "    # Extract the assistant's reply\n",
    "    print(response.choices[0].text)\n",
    "\n",
    "    if userQueries[len(userQueries)-1].lower() == \"exit\":\n",
    "        sys.exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
